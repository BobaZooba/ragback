# ragback

English version in README-EN.md

# My approach

Исходя из ТЗ я предполагал, что данный сервис будет отвечать только за генерацию ответа от LLM. Поэтому здесь нет
хранения Character, User и сообщений. Поэтому здесь нет базы данных и кеша, даже id. Я предполагаю, что за формирование
контекста и хранение всей этой информации отвечает другой сервис. То есть он и будет являться источником инстины

Поэтому контракты и выглядят так:
На вход я принимаю историю сообщений, информацию о пользователе, информацию о персонаже.
На выход отдаю сгенерированное персонажем сообщение и использованные результаты поиска (факты о пользователе).

Как работает:

- По последнему сообщению достаются факты
    - Фильтруются по подобранному трешхолду
    - Именно последнее сообщение, потому что так меньше шума из-за слишком общего эмбеддера
        - В реальности надо учить свой эмбеддер
        - И еще добавить `reranker`
- В промпт добавляется описание персонажа, найденные факты о пользователе (если имеются), имя пользователя, история
  диалога (последние n сообщений)
    - Также в промпте есть дополнительные указания о стиле и формате ответа
- На основе промпта с помощью LLM генерируется следующая реплика персонажа

## Архитектура

Использую Domain Driven Design или по крайней мере значительную его часть. Разделяю на слои:

- `domain` - описание моделей данных, их сервисов и репозиториев (способа хранения данных)
- `application` - описание логики, сервисов, интерфейсы к интеграциям
- `integrations` - реализация интерфейсов интеграций
- `infrastructure` - конфигурация, DI, реализация репозиториев и прочих методов хранения данных
- `presentation` - это реализация способов взаимодействия с данным контекстом

В DDD меня больше всего бесят модели данных и маппинги данных между слоями. Domain <-> infra, domain <-> application,
application <-> presentation. Это занимает много времени, поэтому я сознательно от этого отказался. Не стал использовать
value object для domain слоя в целях экономии времени. В domain и application слоях использовал pydantic. Сознательно не
использовал dto в application слое.

Я не делал репозитории и предполагал, что мой сервис не отвечает за хранение данных.

## Почему 3.12

Protocols - описание интерфейсов. Это позволяет задуматься о реализации сильно позже и (что важнее) не привязываться к
конкретным реализациям (Dependency Inversion). Собственно без этого не мог бы и реализовать DDD. Похожая альтернатива -
абстрактные классы.

Generic - здесь почти не использовал, только в `PromptBuilder`.

## Библиотеки

`rye` - вместо `poetry`. Просто невероятное хорошее решение для менджмента зависимостей, venv и всего прочего. Убирает
бОльшую часть головной боли конлифктом версий питона. На базе `uv`, поэтому быстро загружает зависимости.

Для запуска команд использую `Makefile`. Предпочитаю `justfile` (https://github.com/casey/just), но он менее популярен.

### Code style

- `mypy` - для типизированного питона
- `ruff` - для единого код стайла (вместо `black`)
- `wemake-python-styleguide` - самый строгий и настраиваемый линтер (на базе
  flake8): https://github.com/wemake-services/wemake-python-styleguide
- `pre-commit` - для проверок и для запуска `ruff` перед коммитом

В Github CI настроил только `code style` и `mypy`. Обычно туда добавляю еще тесты и деплой или хотя бы билд докера и
складирование его в нужное место для деплоя.

### Backend

- `fastapi` - для асинхронного API
- `pydantic` - для моделей данных и валидации
- `pydantic-settings` - для настройки конфигов и чтение из `.env`
- `tenacity` - retry policy
- `structlog` - для структурированных логов. Использовал мало и не настраивал, чтобы можно было в `json` хранить для
  экономии времени
- `dishka` - для внедрения зависимостей: https://github.com/reagento/dishka
- `pytest` - использовал бы для тестов, но это было бы уже чересчур для тестового
- `click` - для конфигурирования `cli` тулзов
- `uuid` - использовал бы для id, если бы было что хранить

### LLM

- `mako` - для темплейтов по типу `jinja`
- `openai` - на самом деле использую OpenRouter
- `cohere` - для получения эмбеддингов, потому что у них достаточно неплохие модели
- `qdrant` - векторная база данных. Векторных баз слишком много, преимущества этой: есть все что мне нужно; скейлится
  хорошо; знаю CTO и могу самому себе гарантировать качество его работы; написана на расте, а раст я знаю

### Логирование

Я мало где логировал. Настройка логирования важная тема, но достаточно долгая. Я показал пару примеров логирования,
чтобы убедить читателя, что я знаю что это такое, что надо бы хранить структурированные логи и тд.

### Обработка ошибок

Я сознательно мало обрабатывал ошибки, чтобы сэкономить время. Это достаточно понятная, но кропотливая работа, которая
заняла бы много времени. Ну и все таки это не раст, где обязательно нужно обрабатывать ошибки, а питон, который
позволяет делать так, как есть сейчас.

### Тесты

Я не писал тесты, потому что это выходит за рамки ТЗ. Если бы писал, то использовал бы `pytest`, fixtures, моки, monkey
patching.

# How to run

1. Установите `rye` (https://rye.astral.sh/)
2. Сделайте файл `.env` и заполните его по аналогии с `.env.template`. Понадобится получить ключ от
   OpenRouter (https://openrouter.ai/) и cohere (https://cohere.com/). OpenRouter - это агрегатор LLM от разных
   провайдеров, Cohere нужен для эмбеддинг моделей
3. В терминале введите `make dc.up`, чтобы поднять контейнер с `qdrant` - база векторного поиска
4. В терминале введите `make add-facts`. Эта команда заполняет `qdrant` фактами о пользователе
5. В терминале введите `make run`. Эта команда запускает API сервис
6. В терминале введите `make interact`. Эта команда стартует диалог в терминале. Подождите пару секунд пока бот не
   напишет первое сообщение

Решение работает и без RAG. То есть можно не запускать `qdrant`.

# Character consistently

Какая работа проведена:

- Написание промптов
- Написание описаний персонажа
- Генерация фактов о пользователе
- Ручное тестирование в диалоге и анализ ошибок
- Подбор трешхолда по векторной близости фактов
- Подбор модели

Промпт заточен под ситуацию. Эту информацию можно было бы вынести на уровень персонажа. Еще я бы в API передавал бы
описание ситуации. Тогда получилось бы так: персонаж + пользователь + ситуация. Но ТЗ и время такого не подразумевали.

В реальности я бы еще хранил факты о персонаже.

## Как учитывается ситуация

В промпте явно указано, что нужно использовать разные действия. Примеры: *fist bumps*, *sips drink*, *checks menu* и тд.
Это позволяет лучше погрузиться в ситуацию, не упоминая явно в какой ситуации мы находимся. Таким образом и за счет
промпта модель явно показывает контекст. У модели явно есть возможность совершать небольшие действия и погружать
пользователя в разыгрываемую ситуацию.

## Как персонаж остается консистентным

За счет подобранной модели. Явной проверки не происходит. Ручное тестирование и оптимизация промптов, описаний.

**Из необычного**: промпт оптизировал с помощью Claude. В контекте у Claude был текст тестового задания, текущий промпт,
описание персонажа. Я подавал разные мои диалоги по несколько штук, указывал что мне не нравится и просил переписать
промпт или его часть. Вычитывал получившийся промпт и просил исправить те места, которые меня смущали. Далее опять
общался и подавал диалоги. Таким образом получилось, например, побороть слишком длинные реплики и правильное
использование астериксов (*sips drink*). Постарался сделать так, чтобы промпт не был заточен под определенную модель, а
был полезным и понятным и для других моделей.

## Важный дисклеймер

Для такой задачи опен сорсные модели очень плохо подходят. Нужно тренировать свои. Поэтому оптимизация качества кажется
излишней на данном этапе.

# Metrics

Метрики в таких задачах строятся на основе разметки данных. Для разметки данных нужен либо человек, либо приближение к
человеку (LLM-as-judge). Так что в отличие от других задач мы не можем за секунду посчитать метрики.

Какие бывают метрики? Относительные и абсолютные. Оффлайн и онлайн. Разметка бывает для расчета метрик и для дообучения
моделей. Оценивать можно весь диалог, а можно только один ответ в диалоге.

## Абсолютные

Абсолютные метрики удобны тем, что можно посчитать метрику моделей и понять какая лучше. Звучит прекрасно, но это не
работает в
реальности. Глобальные бенчмарки типа MMLU уже потеряли свою значимость.

Минусов же у таких метрик слишком много. Первый минус - инструкция. Обновили инструкцию? Уже нельзя сравнивать модели
между собой, потому что чуточку, но по разному оцениваются. Не обновлять инструкцию невозможно. Более того она с учетом
продуктовых требований слишком часто меняется. Так что выделенный раннее плюс слишком эфимерный.

Задизайнили шкалу:

- 0 - плохой ответ
- 1 - нормальный ответ
- 2 - хороший ответ

Делаем продукт, сначала доля 0 слишком высока, потом 1, потом 2. Получаем процент двоек скажем 95. Что дальше? Обновим
инструкцию?

- 3 - прекрасный ответ
- 4 - невероятный ответ
- 5 - невобразимо впечатляющий ответ

Так не получится. Каждую категорию вы должны хорошо описать, корнернейсы, чем 3 отличается от 4 и так далее. Ответ может
быть хорош с точки зрения соответствия персонажу, но плох с точки зрения вовлеченности. Окей, давайте сделаем две
метрики. В своих исследованиях раннее я дошел до примерно 50 потенциальных критериев оценки.

В общем, все это сложно задизайнить и переложить в инструкцию, которую кто-то поймет. А потом все равно придется менять.
А в итоге все равно решения принимаются таким: у модели A метрика 73, а у модели B 70. Значит катит на прод модель A.

Зачем тогда нужны абсолютные оценки?

Предлагаю использовать эти метрики для оценки снизу. Скорее это про проверку, а не про улучшения. То есть расти мы можем
много куда и заранее не знаем куда, но мы, как правило, точно знаем чего мы не хотим.

Примеры:

- Общение про суицид
- Предложение встретиться в реальной жизни
- Секстинг

## Относительные

Вкратце: даем человеку (или LLM) два варианта. Нужно выбрать какой лучше. Дальше усложняем: какой вариант лучше с точки
зрения соответствию персонажу? В отличие от абсолютной оценки нам не нужно настолько детально прописывать отличия одной
категории от другой.

Изменения гораздо более заметны. В абсолютной оценке нельзя поставить 2.3, нужно либо 2, либо 3. Тут же такое работает.
Одна модель чуть-чуть лучше другой.

Минусы: мы сравниваем только попарно (или группой). Так что мы не знаем насколько наша текущая модель лучше той модели,
которая была у нас год назад (с учетом что мы переодически релизим новые). Мы можем это посчитать, но это отдельный
замер. Мы предполагаем, что если модель A > B, а модель B > C, то A > C.

## Офлайн оценка

Берем исторические данные, убираем последнюю реплику в диалоге и генерируем ответ новой моделью. Это просто, но есть
нюанс: такой контекст новая модель может не породить. Получается distribution shift. Если итеративно улучшаем продукт,
то влияние такого эффекта уже не так велико.

## Онлайн оценка

Сделали модель, попросили людей с ней поговорить. Плюсы: модель работает на том контексте, который сама и породила.
Минусы: дорого. Автоматизация с помощью LLM пока что опасна (или скорее долго разработать).

## Решение

### Оценка модели на проде

Берем данные за последнюю неделю. Делаем из них диалоги длиной примерно 20 сообщений и с окном в 5.

#### Moderation

Случайные диалоги отправляем на LLM-as-judge с тегами наличия сообщений от модели:

- Hate speech
- Sexual
- Self-harm
- Violence
- Ошибки (грамматические и тд)

Можно также использовать https://platform.openai.com/docs/guides/moderation

Затем можно будет обучить на этом классификатор и проверять бОльших объем.

#### Improvements

В ТЗ сказано, что надо оценивать две вещи:

- “In-character” consistency
- Scenario focus

Тут очень хочется поспорить почему именно это надо оценивать. Будто бы продуктово гораздо важнее оценивать вовлеченность
в
диалог. Это ведь ведет к более длительной сессии. И еще никак не учитываем аспекты связанные с преподаванием.

Нам не так важно, что наша модель получила 76 попугаев в бенчмарке “In-character” consistency. Нам важно стали ли мы
лучше со временем. Так что мы будем сранивать с диалогами, которые были с предыдущей версией модели. Тоже возьмем
исторические данные, также порежем на 20 сообщений.

Дальше у каждого диалога должны быть данные какого персонажа модель отыгрывала и в какой ситуации.

Пример:

```
Персонаж 1
Диалог 1

Персонаж 2
Диалог 2

Какой из диалогов лучше отыгрывает своего персонажа: 1, 2 или ничья?
```

Инструкция конечно должна быть больше и покрывать больше кейсов. Что значит лучше отыграть одного или другого персонажа?
Такая инструкция делается на основе реальных диалогов.

Получаем 3 оценки:

- Насколько модель B лучше модели A соответствует персонажу
- Насколько модель B лучше модели A соответствует ситуации
- Насколько модель B лучше вовлекает в диалог, чем модель A

Где модель B - это новая модель, которая последнюю неделю отвечает в проде, а модель A - это модель, которая отвечала
раньше в проде.

Если новая модель где-то проигрывает, то надо переходить к анализу ошибок (что является таким же большим топиком как и
расчет метрик) и работать над моделью C. Модель C надо будет сравнить также и с моделью B. Нужно ли откатывать до модели
A является спорным моментом.

### Катить ли новую модель?

Берем также исторические данные, убираем последнюю реплику, генерим новой моделью. Нужно оценивать разные места в
диалогах, начало, середина, конец, то есть должен быть разный размер контекста. Дальше оцениваем насколько новый ответ
лучше старого по критериям выше. Ожидаем, что мы оптимизируемся примерно на одном и том же потоке данных, поэтому
контекст от новой модели будет +- таким же как и от старой и мы не сильно себя обманываем.

Если получаем прирост (и не ухудшаемся по каким-то другим критериям), то выкатываем на часть пользователей.
